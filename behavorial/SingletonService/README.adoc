== Singleton Service

In this example we see see how a PodDisruptionBudget works.
As the other examples we  assumes a Kubernetes installation available.
Check the link:../../INSTALL.adoc#minikube[INSTALL] documentation for the installation how you can use an online Kubernetes playground.
We can't use Minikube in this example because we need a 'real' cluster from where we can drain a node to see the effect of a PodDisruptionBudget.

First, let's create a Deployment with six Pods:

[source, bash]
----
kubectl create -f https://k8spatterns.io/SingletonService/deployment.yml
----

We can check on which nodes the Pods are running with

[source, bash]
----
kubectl get pods -o=custom-columns=NAME:.metadata.name,NODE:.spec.nodeName
----

For being sure that always four Pods are running, we create a PodDisruptionBudget with

[source, bash]
----
kubectl create -f https://k8spatterns.io/SingletonService/pdb.yml
----

Now let's drain a node and see how the Pods are relocated.
Let's say that the second node is called `node01` (that's the case for a Katakoda setup)

[source, bash]
----
kubectl drain --ignore-daemonsets node01 >/dev/null 2>&1 &
----

We do the removing of the node in the background, so that we can watch how the Pods are relocated:


[source, bash]
----
watch kubectl get pods -o=custom-columns=NAME:.metadata.name,NODE:.spec.nodeName
----

As you can see, there are always at least four Pods running on both nodes until eventually all six Pods are running on the master node.

You can undo the drain operation with

[source, bash]
----
kubectl patch node node01 -p '{"spec":{"unschedulable":false}}'
----

and restore the Deployment with

[source, bash]
----
kubectl scale deployment random-generator --replicas 0
kubectl scale deployment random-generator --replicas 6
----





For the rest of the example we are watching the Pods in the console by putting the following command into the background

[source, bash]
----
kubectl get pods -w &
----


Let's switch to version 2.0:

[source, bash]
----
kubectl set image deployment random-generator random-generator=k8spatterns/random-generator:2.0
----

You will remark that first all 1.0 Pods are stopped before the new Pods with version 2.0 are started.

Now let's introduce a PodDisruptionBudget with the following commands.

This will ensure that always two Pods are running.
We can check this by switching back to version 1.0 of our image and watching the Pods stopping and starting

kubectl set image deployment random-generator random-generator=k8spatterns/random-generator:1.0



The PDB use Pod selector which maches on the labels `app=random-generator`

Let's create such Pods with
